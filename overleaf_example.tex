% 完整的 Overleaf 文档示例（ACL 格式）
\documentclass[11pt,a4paper]{article}

% ACL 会议通常使用这些包
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% 双栏格式
\usepackage{two-column, coling}
% 或者使用标准的双栏格式：
% \usepackage[twocolumn]{geometry}

% 图片路径设置（根据你的文件结构调整）
\graphicspath{{./}{images/}}

\begin{document}

\title{Your Paper Title}
\author{Your Name}
\maketitle

\begin{abstract}
Your abstract here.
\end{abstract}

\section{Introduction}
Your introduction text here.

\section{Model Architecture}

We use ELECTRA-small \cite{clark2020electra} as our backbone model. ELECTRA is a pre-trained language model that shares BERT's architecture but uses a more efficient replaced token detection objective during pre-training. We chose ELECTRA-small over larger models like BERT-base for practical reasons: it achieves competitive performance while being significantly faster to train.

The model consists of 12 transformer layers with 256 hidden dimensions and 4 attention heads, totaling approximately 14 million parameters. For the NLI task, we add a classification head on top of the \texttt{[CLS]} token representation. Figure~\ref{fig:electra_arch} illustrates the model architecture.

% 双栏图片：使用 figure* 环境
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{electra_arch.png}
    \caption{ELECTRA-small architecture for NLI. The model processes the concatenated premise and hypothesis through the ELECTRA-small backbone (12 transformer layers), extracts the \texttt{[CLS]} token representation, and applies a classification head to produce the final prediction.}
    \label{fig:electra_arch}
\end{figure*}

\section{Experiments}
Your experiments section here.

\section{Conclusion}
Your conclusion here.

\begin{thebibliography}{}
\bibitem{clark2020electra}
Clark, K., Luong, M. T., Le, Q. V., \& Manning, C. D. (2020). Electra: Pre-training text encoders as discriminators rather than generators. \textit{ICLR}.
\end{thebibliography}

\end{document}

